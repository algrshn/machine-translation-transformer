# Coding a transformer model from scratch and training it on UN Parallel EN-FR corpus
This is a pure training exercise. I wanted to code a transformer as described in [*Attention is All You Need*](https://arxiv.org/pdf/1706.03762.pdf) paper and train it on a high quality parallel corpus. 
